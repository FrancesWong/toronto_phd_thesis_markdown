
# Introduction

## The engram

### A history of ideas
If we accept the philosophical (and now scientifically incontrovertible) position that all activity of the mind is mediated by activity of the brain, then it follows that whenever an event is stored in mind such that it can be recalled at a later time (ie. a memory), there is some concurrent change in the biological substrate of the brain. To understand how such changes or ‘engrams’ occur, and how they give rise to the gift of memory that shapes our thoughts, beliefs, and behaviours is one of the great goals of neuroscience. Many scientists have devoted their careers to studying this question. Among them, the Canadian neurophysiologist Donald Hebb stands out for his compelling theory of how information could be encoded into cell assemblies [@hebb_organization_1949]. Hebb was not the first to propose the concept of the engram, the German biologist Richard Semon had done so years earlier [@semon_mneme_1921], but he advanced the field greatly by presenting a theory of memory that drew upon all the neurophysoiological data that had been collected at that time. His thesis, remembered today in the simplified adage “neurons that fire together, wire together”, was built on the concept that changes in the strength of connections between neurons lead to the formation of cell assemblies, groups of cells whose coincident activation give rise to certain behaviours or perceptions. Such cell assemblies, formed during the encoding of a memory, increase the chance that the same pattern of activity could be recreated at a later time and trigger memory recall.

Hebb’s postulate arrived at a time when many neuroscientists  believed that the brain carried out many of its functions holistically, with no particular area of the brain or group of cells being more important than any other. Indeed, Hebb’s own doctoral advisor Karl Lashley had come to this conclusion after his systematic attempt to locate the engram in 1917 had failed. Lashley had examined the effects of lesions in various cortical areas on the retention of a maze and inclined plane learning task in rodents [@bruce_fifty_2001]. He failed to find any specific region of the cortex that, when lesioned, would erase memory. Instead, he only observed memory impairments after large areas of the cortex were damaged, regardless of location. This lead him to propose that memory was broadly distributed throughout ‘equipotential’ neural circuits and that finding the engram was impossible. This belief would provide a source for debate between the two thinkers and within the scientific community until Lashley’s death.

It was not until technologies for probing and manipulating the activity of the brain became sufficiently precise that Hebb’s ideas could be validated. First, more detailed more detailed lesion studies in mammals proved that subcortical areas of the brain were necessary for memory storage [@mishkin_memory_1978, @morris_place_1982-1, @weiskrantz_behavioral_1956]. Additionally, the first evidence for a discrete molecular change underlying a simple form of learning was discovered in the flatworm Aplysia  [@kandel_cell-biological_1978]. Most importantly, the discovery of long-term potentiation (LTP) [@bliss_long-lasting_1973] provided the strongest vindication for Hebb’s theory that memories are stored by durable changes in neural connectivity. LTP refers to a phenomenon in which high frequency activity across a synapse increases the strength of that synaptic connection [@malenka_ltp_2004]. The scientists who discovered LTP wisely left open the question of whether the phenomenon, which was identified in cultured hippocampal cells, could be used by living organisms for memory storage. However, LTP, especially when considered alongside the phenomenon of spike-timing-dependent plasticity, wherein coordinated firing between pre- and post-synaptic neurons drives the induction of plasticity [@markram_regulation_1997], corresponds strikingly well with the mechanisms described by Hebb.

### Observing the engram
More recently, tools that allow neurons active during the time of memory formation to be identified and tagged for future manipulation have been able to answer questions about memory information beyond the level of the synapse. By analyzing the activity of groups of neurons, it has been possible to demonstrate how neurons activated during memory encoding form neuronal ensembles that underlie memory retrieval. Furthermore, these techniques has important questions at the heart of Lashley and Hebb’s debates to be answered: to what degree are engrams distributed throughout the brain?

Strategies for identifying the engram have often relied on immediate early genes (IEGs) such as Fos, Zif268, or Arc. These genes are reliably induced by neural activity [@guzowski_mapping_2005] and return to baseline low levels of expression within minutes. Pairing IEGs with expression of molecular tags such as green-flourescent protein (GFP) or LacZ allows researchers to permanently label populations of cells active during learning. By combining these permanent tagging approaches with methods to detect IEG expression induced by memory retrieval, researchers have been able to test one key hypothesis made by engram theorists: neurons active during the experience of an event are reactivated during the memory of that event. Several studies have now shown that there is a high degree of overlap between the populations of neurons active during memory encoding and recall [@denny_hippocampal_2014, @reijmers_localization_2007, @tayler_reactivation_2013]. Although this pattern of reactivation could be traced to specific neurons, those neurons were distributed broadly throughout both cortical and subcortical areas of the brain [@denny_hippocampal_2014, @tayler_reactivation_2013]. Therefore, the engram of a memory appears to involve a widely distributed collection of neurons distinguished on the basis of their activity during the events that lead to the formation of that memory. 

### Research showing necessity
Studies that merely observe the engram are unable to demonstrate more than a correlational relationship between brain changes and memory. Thus, many studies have made us of techniques to ablate or silence the neuronal ensembles tagged during memory formation to demonstrate the necessity of their activation for memory retrieval. In addition to proving that the engram resides in specific cells, these studies have been able to demonstrate the importance of brain regions such as the amygdala and hippocampus in storing and retrieving memories. 

By selectively introducing inhibitory or toxic receptors into neurons that undergo engram-related changes, many studies have demonstrated that preventing the reactivation of engram cells in specific networks of the brain during re-exposure to memory cues prevents memory recall. Either through selective genetic ablation [@han_selective_2009], inhibitory optogenetics [@tanaka_cortical_2014], or DREADDS [@hsiang_manipulating_2014], when those neurons that were most active during learning are prevented from firing, memory retrieval is either impaired or eliminated. In comparison to Lashley’s lesion experiments, these studies succeeded by specifically targeting subcortical brain regions known to be involved in each particular form of learning under investigation. Furthermore, they were able to target those specific neurons that had presumably undergone changes in synaptic connectivity during encoding.
The ability of localized inhibition to interfere with the reactivation of a widely distributed engram demonstrates the importance of certain regions in initiating memory retrieval. For example, the silencing of only a small subset of cells in the CA1 region of the hippocampus [@tanaka_cortical_2014] was sufficient to wipe out a memory that involved distributed representations throughout the brain. This results suggest that distinct areas of the brain, especially those involved in processing contextual or sensory input such as the CA1 or the lateral nucleus of the amygdala, may contain critically important neuronal ensembles  that ‘trigger’ a broad pattern of reactivation across the brain when activated by appropriate retrieval cures. This idea has been supported by the finding that directly reactivating cortical engram neurons is able to bypass the effects of subcortical inactivation [@cowansage_direct_2014].

### Research showing sufficiency

In order to demonstrate the sufficiency of engram reactivation for memory retrieval, many researchers have focused their efforts on artificially activation neuronal ensembles formed during learning. In one study, artificially inducing the firing of hippocampal neurons tagged during the formation of a fearful memory was shown to increase the amount of time mice displayed fearful behaviour when placed into a completely novel context (@liu_optogenetic_2012). Many studies have since demonstrated that artificial activation of engram neurons in both the hippocampus and amygdala can trigger of memory recall in the absence of appropriate retrieval cues [@garner_generation_2012-2, @kim_creb_2013, @yiu_neurons_2014]. Such artificial memory activation has also been shown to trigger reactivation of widely distributed populations of neurons outside the area of stimulation, such as would be observed during natural memory retrieval [@cowansage_direct_2014].

Studies involving artificial reactivation of memories have been able to go further than simply demonstrating the necessity of engram reactivation for memory retrieval. For example, artificially reactivated memories have been shown to have effects on long-term emotional behaviour [@ramirez_activating_2015]. Furthermore, intense artificial activation of engram neurons in the amygdala or hippocampus during exposure to new contexts is capable of modifying engrams and producing false memories [@ohkawa_artificial_2015, @redondo_bidirectional_2014]. This is a compelling area of current research that provides the strong evidence that memories are mediated by the activity of discrete ensembles of cells and may one day permit a greater understanding of how the contents of memory are encoded.

### Memory allocation

Understanding how neuronal ensembles are formed during memory formation should provide insight into how information is stored in an engram. Memory allocation refers to the processes by which specific cells and synapses are selected to undergo changes that will become part of the engram. The neural mechanisms by which this occurs have not been specifically defined, but both theoretical and experimental work has proposed that memory allocation is driven by a process of sparse coding that provides efficient information storage.

A critical component of the engram supporting an auditory fear memory can be localized in the lateral nucleus of the amygdala (LA). In the LA, many different sensory input streams converge, allowing associations to be formed between stimuli and the expectation of threat or reward [@ledoux_emotion_2000-1]. Several experiments exploring auditory fear conditioning in the LA have demonstrated that neurons expressing high levels of the transcription factor CREB are more likely to be allocated to memory ensembles [@hsiang_manipulating_2014, @han_selective_2009, @han_neuronal_2007]. CREB is a necessary component of the synaptic plasticity pathways that underlie long-term potentiation [@silva_creb_1998], but also increases intrinsic neuronal excitability (@zhou_creb_2009). Thus, excitability has been proposed to play an important role in memory allocation, with more excitable cells more likely to be allocated to an engram in the LA. Recent studies have confirmed this hypothesis, showing that manipulating cellular excitability directly without altering CREB expression also increases the likelihood that neurons will become part of an engram (@yiu_neurons_2014).

However, artificially increasing the excitability of LA neurons in these studies did not increase the overall size of the of the LA component of the engram (in terms of number of neurons). This suggests that memory allocation is a competitive process determined by relative rather than absolute excitability [@han_neuronal_2007; @yiu_neurons_2014]. In addition, a wide range of studies involving different analytical techniques has repeatedly shown that only a small, sparse proportion of principal neurons in the LA become part of any one fear memory trace (@an_long-term_2012, @ghosh_neuronal_2015, @gouty-colomer_arc_2015, @herry_switching_2008-1, @quirk_fear_1995, @reijmers_localization_2007, @rumpel_postsynaptic_2005] despite the fact that more than 70% of neurons in the LA receive the appropriate sensory innervation (@repa_two_2001, @romanski_somatosensory_1993]. Together, these results suggest that the number of neurons involved in representing sensory events remains constrained to a sparse proportion of neurons despite variations in excitability and sensory input. The concept of small, consistently-sized representations corresponds with theories of sparse coding and data collected from other brain regions [@hromadka_sparse_2008, @sanes_plasticity_2000, @weliky_coding_2003, @wixted_sparse_2014]. Sparse distributed coding, in which discrete units of information are encoded across small subsets of neurons in a large network, is thought to provide a structure for high-capacity memory storage that is robust to noise and capable of being implemented in a rapidly-changing biological substrate [@ahmad_properties_2015, @druckmann_neuronal_2012, @krieg_unifying_2014-1]. Memory allocation in the LA may proceed in such a way as to promote sparse coding.

## The amygdala 
### The Epicenter of conditioned emotional behaviour

Research in psychology and neuroscience has made great progress towards understanding how the brain learns and remembers by focusing its efforts on simple forms of learning such as sensitization and Pavlovian conditioning. These simple models of memory provide the dual advantage of allowing research to be performed in animal models and proceeding through relatively simple neural mechanisms that permit underlying features of neural learning to be identified more rapidly.

Famously, Ivan Pavlov identified a form of simple learning that could be described by a formalized system of stimulus association and conditioning. While studying gastric physiology in dogs, he discovered that they would often began to salivate as soon as he entered the room, even if he had no food to feed them with. This inspired him to perform his classic experiment, in which he paired feeding with the ringing of a bell and recorded the amount of saliva the dogs eventually produced after hearing the bell alone. He observed that repeated pairing of the bell with food delivery created an association between the two stimuli, such that the presentation of the bell was sufficient to produce the behavioural response that would normally only accompany presentation of food. He formalized this process with the framework of classical conditioning, describing how some stimulus that ordinarily has a neutral effect on an organism, the unconditioned stimulus (US), can become associated with a stimulus that produces an innate or instinctual response (CS) with repeated pairing.

Pavlovian conditioning has formed the basis of much of modern memory research. In animal models, Pavlovian conditioning is widely used due to the ease of quantifying behaviour with measures such as freezing (an immobile posture indicative of prey behaviour performed to avoid detection by predators). The formation of both fearful and rewarding associations with explicit auditory and visual cues has been shown to depend on the activity of the amygdala, a small subcortical region of the brain that is highly conserved across species [@janak_circuits_2015]. 

The amygdala is important for emotional processing and the expression of has been shown to be critical for emotional processing, including the detection of motivationally-salient stimuli in the environment and the expression of emotions such as fear and anxiety. There are two amygdali, situated on either side of the brain in the medial temporal lobe, and although the amygdala is often referred to as a single structure, it can be deconstructed into ~13 distinct subnuclei based on cytoarchitecture and histochemistry. For simplicity, the amygdala is commonly discussed as consisting of the lateral (LA), basal (BA), and central nucleus (CeA) [@sah_amygdaloid_2003]. These nuclei, collectively referred to as the amygdaloid complex, are separated by small intercalated masses of inhibitory cells (ITCs) that gate the flow of information through the complex, which primarily flows from the LA, through the BA, and either into the CeA, which drives fear and anxiety behaviour related to negative memories, or the nucleus accumbens (NAc), which governs motivation and reward [@ehrlich_amygdala_2009, @lee_inhibitory_2013].

![**Major nuclei of the amygdala.** The amygdaloid complex with the major subnuclei and patterns of connectivity labeled. During memory recall, sensory input is received by the LA, which sends excitatory projections to both the BA and CeA. The BA, which also receives input from the prefrontal cortex and hippocampus (not shown) projects to the CeA in the case of fearful associations, or to the NAc for rewarding associations. The CeA drives fear and anxiety related behaviour through its connection with the BNST. ITC clusters situated between the major subnuclei regulate the flow of information through the circuit \label{ref_a_figure}](source/figures/amygdaloidcomplex.jpg)


### The lateral amygdala

The horn-shaped lateral amygdala is the most dorsally-situated nucleus of the amygdala and is bordered ventrally by the basal nucleus, laterally by the external capsule, and medially by the central nucleus. The LA receives the majority of the sensory projections sent to the amygdala and is thought to function as the amygdala’s sensory interface [@ledoux_lateral_1990]. Somatosensory, gustatory, auditory, and visual cortices all send heavy projections to the LA, as do regions of the thalamus that carry corresponding information. During auditory fear conditioning, auditory information reaches the LA through both associative auditory areas and the medial geniculate nucleus (MGN) of the thalamus. These two input streams are thought to represent two distinct pathways of information: one carrying complex, slow information from the cortex and one carrying early, but simpler signals from the thalamus [@bergstrom_organization_2014]. This pattern is similar for visual information, with both cortical and thalamic sources of information synapsing in the LA [@shi_visual_2001]. The LA also receives minor innervation from the prefrontal cortex and hippocampal formation (entorhinal cortex and subiculum).

The LA projects to most other regions of the amygdaloid complex, including the basal nucleus, central nucleus, and intercalated cell regions. Interestingly, all of these regions send moderate to heavy input back to the LA, suggesting that most of these connections are reciprocal [@pitkanen_connectivity_2000]. The purpose that this reciprocal connectivity with the other subnuclei of the amygdala is unknown

The majority of neurons in the LA (~70%) are cortex-like, spiny pyramidal neurons similar to those found in hippocampus or cortex. However, unlike in those regions, these neurons are randomly organized without any parallel organization in any plane and are naturally quiescent [@sah_amygdaloid_2003]. The remaining neurons primarily consists of a heterogenous population of non-pyramidal local circuit interneurons that provide local inhibition through GABAergic synapses [@spampanato_interneurons_2011].

### Subnuclei of the lateral amygdala

Although many studies have investigated the nuclei of the amygdala as if they were uniform entities, classic tracing and electrophysiology studies have identified important differences between small subregions, including the LA [@sah_amygdaloid_2003]. The LA has been proposed to consist of three distinct subnuclei: dorsal, ventral-lateral, and ventral-medial. These regions display different patterns of connectivity to targets both within and outside the amygdala. However, how these differences affect information processing during the formation and retrieval of memories is largely unknown and unexplored.

The dorsal subnucleus of the lateral amygdala (LAd), situated at the tip of the LA’s horn-like structure receives the majority of the LA’s thalamic sensory inputs as well as containing the greatest degree of overlap between auditory and somatosensory innervation [@pitkanen_intrinsic_2003, @romanski_somatosensory_1993]. The LAd also receives the least amount of reciprocal projections from other nuclei of the amygdala. One study using single-unit recording to measure the activity of single neurons in the LA reported that neurons in the LAd demonstrate the earliest changes in responsiveness to stimuli during fear conditioning [@repa_two_2001]. These early-responding neurons underwent rapid, but transient potentiation during fear learning that decayed as memories became more stable over a number of training events, a finding that was replicated in computational models [@kim_assignment_2013-1]. These observations lead the authors to propose that these LAd neurons may be involved in a process of amygdalar memory consolidation dependent on the LAd. However, empirical research since this discovery has failed to replicate or expand upon these findings. 

The other two subnuclei of the LA, the ventral-lateral (LAvl) and ventral-medial (LAvm) also receive sensory input, but are distinguished from the LAd by differences in the source of these inputs and in their connectivity to other regions of the amygdala. The LAvl and LAvm both receive local input from the LAd, but do not innervate each other, suggesting that information flows unidirectionally through the LA [@pitkanen_organization_1997]. These subnuclei also receive the majority of the reciprocal connections returned to the LA from the BA and CeA. Generally, the sensory input that the LAvl and LAvm receive is more cortical than thalamic in origin. In particular, the LAvm receives most of its input from regions related to higher-order processing, such as the prefrontal and perirhinal cortex and the hippocampus [@pitkanen_organization_1997].

Despite the differences that have been demonstrated between these subnuclei, it is difficult to draw conclusions as to how they interact and contribute to memory function. For example, some inputs, such as from the dorsal perirhinal cortex, have very little selectivity [@shi_perirhinal_1999]. Furthermore, the high degree of connectivity between these regions suggests that, even if sensory input is initially received by a non-overlapping populations of neurons, it may later be integrated through pathways that link these subnuclei.


### The basal amygdala

The basal nucleus of the amygdala is located ventrally to the LA and is thought to integrate input from the LA related to sensory stimuli with information related to context and cognitive processes. Similar to the LA, the BA can be subdivided into distinct subregions: the rostrocellular, magnocellular, and intermediate subnuclei [@sah_amygdaloid_2003]. The BA is highly connected with hippocampal and prefrontal areas [@amano_fear_2011-1]. In addition, the BA contains the majority of the amygdala’s projections to the nucleus accumbens that mediate the expression of rewarding memories [@beyeler_divergent_2016].
 
Within the amygdaloid complex, the BA receives many large projections from the LA and sends inputs back to the LA and CeA. The BA also receives projections from thalamic and cortical sensory areas, but the majority of its extra-amygdalar inputs come from the prefrontal cortex and hippocampus, which are thought to contribute to the basal nucleus’ role in mediating the extinction and context dependency of learned associations [@herry_switching_2008-1].

The BA contains a similar variety of neurons as the LA as distinguished by morphology. However, neurons in the BA have been shown to have a significantly higher baseline rate of firing than the LA [@sah_amygdaloid_2003]. 


### The central amygdala

The central nucleus of the amygdala is located in the dorsomedial region of the amygdaloid complex and consists of two distinct subdivisions, lateral and medial. These subdivisions engage in a great deal of interplay during memory formation and retrieval, with the lateral subdivision receiving the majority of the inputs from the LA and BA and regulating the activity of the medial subdivision [@ehrlich_amygdala_2009]. The CeA is thought to function as the output center of the amygda for fear and anxiety-related behaviour. During retrieval of a fear memory, connections from the medial subdivision of the central amygdala activate the nearby bed nucleus of stria terminalis (BNST), which then activates downstream arousal and anxiety-producing regions of the hypothalamus [@kim_diverging_2013-1].

The morphological organization of the CeA is very different from that of the other amygdaloid nuclei. This is likely due to their different embryological origins, with the CeA sharing its developmental origin with the primarily inhibitory striatum and the LA and BA developing alongside excitatory, cortical regions [@sah_amygdaloid_2003]. Unlike the LA and BA, the CeA is primarily inhibitory and GABAergic, with abundant local connections between subdivisions and a wide variety of interneurons, some of which have been shown to play distinct roles in anxiety [@botta_regulating_2015].

### Intercalated cell regions

The intercalated cell masses are groups of specialized GABAergic interneurons located in between the lateral and basal nuclei and the central nucleus. Although not much is known about these intercalated cells, they have been shown to provide both feed-forward and feed-back inhibition to the other nuclei of the amygdala and are thought to function as an inhibitory gate that regulates the flow of information between the input and output interfaces of the amygdala [@pape_gabaergic_2005].
 
## Inhibitory Interneurons

The amygdala, much like any other brain region, is highly dependent on an appropriate balance between inhibitory and excitatory neural activity. Throughout the amygdaloid complex, there is a wide variety of inhibitory interneurons with various physiological properties and patterns of connectivity [@ehrlich_amygdala_2009, @sah_amygdaloid_2003, @spampanato_interneurons_2011, @waclaw_developmental_2010]. In the LA and BA, these interneurons comprise about 20% of the neuronal population and have been shown to be central to the process of fear conditioning [@ehrlich_amygdala_2009], coordinating the flow of information between nuclei and gating sensory input through finely-tuned systems of inhibition and disinhibition [@wolff_amygdala_2014]. These interneurons can be distinguished on the basis of the presence of different molecular markers, including parvalbumin (PV), somatostatin (SOM) and cholecystokinin (CCK), which are strongly correlated with different functional characteristics.

### Parvalbumin-containing Interneurons

Interneurons that express the calcium-binding protein parvalbumin are prevalent (~15% of neurons) in the lateral and basal nucleus of both the rodent and human amygdala [@sorvari_parvalbumin-immunoreactive_1996]. PV^+^ cells display a heterogeneity of firing properties [@rainnie_physiological_2006], but generally share the same morphological characteristics. PV^+^ interneurons almost always form synapses on the cell bodies of principal neurons, where they are able to effectively modulate neural excitability and mediate synchronous oscillations [@freund_perisomatic_2007]. PV^+^ interneurons primarily receive input from local excitatory neurons. However, they have been shown to be highly sensitive to sensory input from both thalamus and cortex, suggesting that they may receive cortical or thalamic innervation as well [@szinyei_putative_2000, @woodson_afferents_2000].

### Somatostatin-containing Interneurons

Somatostatin-containing interneurons represent a similar proportion of the interneuronal population as those expressing PV [@sosulina_gabaergic_2010]. SOM^+^ interneurons display a distinct delayed, stuttering pattern of firing in response to stimulation. Importantly, SOM^+^ neurons form synapses on the distal dendrites of pyramidal neurons rather than on the cell body, which lends them the ability to selectively inhibit input arriving on individual dendritic branches rather than manipulate whole-cell excitability.

As the dominant interneuronal subtypes in the amygdala and hippocampus, SOM^+^ and PV^+^ interneurons interact with each other to a large extent. It has been suggested that PV^+^ interneuron-mediated inhibition of SOM^+^ cells gives rise to a process of disinhibitory sensory control that is involved in the formation of associative memories in both the hippocampus [@lovett-barron_regulation_2012] and LA [@wolff_amygdala_2014]. In both these regions, early activation of PV^+^ interneurons has been proposed to inhibit downstream SOM^+^ interneurons, thereby enhancing responsiveness to sensory information delivered to the distal dendrites of pyramidal neurons and promoting the excitability necessary for plasticity and allocation to the engram.

### Cholecystokinin-containing interneurons

Interneurons in the LA that express cholecystokinin are perhaps the most diverse in terms of their functional properties [@jasnow_distinct_2009]. For example, they have been shown to demonstrate both adapting and non-adapting firing, high and low input resistance, as well as both fast and slow after-hyperpolarization periods. This variety may be related to the fact that mRNA for CCK is more broadly distributed throughout the region than any other molecular marker used to distinguish interneuronal subtypes. In the amygdala, not much is known about the functional role of CCK^+^ interneurons. However, there has been some suggestion that a distinct subtype of CCK^+^ cells may be specifically involved in mediating anxiety related behaviour [@truitt_anxiety-like_2009] .

## Modeling memory

In order to understand a system as complex as the brain, it is necessary to develop models that present simplified versions of phenomena. By reducing the amount of variables in a model to those that are specifically relevant to the function or task at hand it becomes much easier to examine the relationship between relevant factors and test the predictive or explanatory value of theories about how a system works. Such models can be especially useful especially useful when they create novel predictions that can then be tested experimentally. Since the time of Donald Hebb, theoretical models in the field of learning and memory have proven their usefulness for understanding how changes in the brain give rise to memory.

Most computational models used to study learning and memory belong to a class of models known as ‘neural networks’. These networks are based off of nodes, simplified mathematical abstractions of neurons that are connected to each other through connections (often referred to as edges) that represent simplified synapses. Each node in a neural network has an activation level that is determined by the input into that node from outside the network, or from other nodes in the network. A node’s activation level can be thought of as the likelihood that it will generate an output of its own or, alternatively, how active a neuron is on a scale from 0 (not active) to 10 (fully active). The edges in the neural network transmit this activation level between nodes, with the amount of activity that is sent from the output node to the input node determined by that edge’s synaptic weight. Importantly, weights can have both positive and negative values, corresponding to excitatory and inhibitory connections in the brain (although some models allow nodes to have output edges with both positive and negative weights, which is biologically inaccurate). Each set of nodes that sends output to another set of nodes is referred to as a layer. The result of this simple architecture is that each node is capable of computing a weighted sum from a number of inputs. Although this low-level operation is very limited on its own, as more nodes (and layers of nodes) are added to a neural network, and as rules are implemented that allow the connections between nodes to be modified, their computational power becomes increasingly powerful. In addition to being able to solve computationally ‘hard’ problems such as mastering the game of Go (Silver et al., 2016), Neural networks have been used to successfully model many forms of learning and memory, from Pavlovian conditioning [@fanselow_pavlovian_1998]  to systems-level consolidation of long-term episodic memories [@santoro_computational_2015]. 

### Autoassociative neural networks

Hebb’s original prediction for how memories could be stored in the brain involved the simple rule that if cell A connects to a second cell B, and if the two cells are repeatedly active at the same time, then the connection between them is strengthened, so future activity in A is more likely to activation in B [@hebb_organization_1949]. This rule, now known as Hebbian learning can be clearly replicated in neural networks in which the strength of connections between nodes is increased if they are active at the same time.
 
One class of neural network that implements Hebbian learning are autoassociative netoworks. The distinguishing characteristic of these networks is that each node is both an input and an output node, meaning that it both receives input from external sources and contributes to the overall output of the nework by transmitting its activity to other nodes [@gluck_gateway_2001]. When a pattern of input activates an autoassociative network, some subset of nodes are activated by that external input, while others remain silent. Which neurons are activated is determined both by the connectivity and weights of external inputs and, because nodes are able to excite each other, pre-existing weights between nodes in the network (Figure 1.2a). If a novel pattern of external input is received, Hebbian learning in the autoassociative network ensures that the synaptic weights of edges between coactive nodes are strengthened such that the subset of neurons activated by external input are bound together into an ensemble, storing this pattern in the network (Figure 1.2b,c). Later, if an incomplete or distorted version of the same input is given to the network, this pattern is more likely to be reinstated due to the enhanced connectivity between those neurons that were most excited by the original input. This process, known as pattern completion, is thought to play an important role in the brains’ ability to recognize patterns in our environment and recall previous sensory representations during classical conditioning (ie. if a tone and shock are presented simultaneously, nodes that respond selectively to either stimulus will be linked such that later presentation of either tone or shock alone will lead reactivation to the entire population that was active when both tone and shock were presented).

![**Autoassociative memory.** Illustration of autoassociative memory encoding and retrieval. (A) Sensory input (not shown) to the network activates a certain subset of nodes (solid circles). (B) Hebbian learning leads to the strengthening of the weightened connections between nodes that are activated by sensory input, storing this pattern of activity. (C) Later, a different new pattern of sensory stimuli evokes the activation of a different subset of nodes in the network. (D) Weights between nodes active during this new pattern are strengthened and (E) both patterns are now stored in the network (heavy lines). (F) If a partial version of a previously stored pattern is presented to the netowork, some of the nodes involved in that pattern will be activated. (G) Activation will then flow along previously strengthened connections and lead to (H) the reactivation of the previously stored pattern. Adapted from Gluck and Myers, 2001. \label{ref_a_figure}](source/figures/autoassociative.jpg)

### Sparse distributed memory

The theory of sparse distributed memory is a mathematical model that attempts to describe how memory could be implemented in the brain with a formal, quantified approach. Sparse distributed memory does not require neural networks (indeed, it has been widely implemented in the world of traditional computer science), but it is very easily implemented by neural networks, including autoassociative networks. The central principle of sparse distributed memory is the statistical recall of previously active patterns distributed across a range of different locations in a very large network [@kanerva_sparse_1988].

In order to understand sparse distributed representations, it is best to draw comparisons with other coding schemes such as local coding. In a local coding, every unit of information is stored at a single location with no overlap between units. A common analogy to this is a computer keyboard, where each key matches to a character in a one-to-one fashion. In neuroscience, local coding is expressed to the extreme by “Brad Pitt neuron” hypothesis. In this hypothesis, there exist neurons that store memories for only one particular person, place, or thing. When, for example, an image of Brad Pitt is shown, the recognition of Brad Pitt is mediated entirely by the firing of a single “Brad Pitt neuron”. Local representations have the advantage of being easy to decode (the memory is always in one location) and they avoid the risk of interference between memories. However, they have extremely low capacity, as every memory requires its own dedicated location, and they are inflexible, as new memory addresses would need to be added to permit new learning. Furthermore, because memories are disconnected from each other, such representations do not allow memories to be linked or generalized in any way. This lack of flexibility would be completely untenable for an organism required to navigate a noisy world of fuzzy inputs.

Another example of a coding scheme that is inefficient within in the hardware of the brain is dense coding. This is the form of memory implemented by computer binary that stores information as patterns distributed across the entirety of a fixed number of locations. For example, in ASCII code, each letter is encoded as a string of 8 binary values. When this string is read, the computer then reads each bit and performs a lookup operation to determine which character corresponds to the string that has been read (01100001 = a, 01100010 =b, etc.). Such a coding scheme is convenient for use in computers because only a small number of bits need to be read to encode a large number of distinct memories (8 bits = 256 possible combinations). However, there are significant drawbacks to this scheme outside of the orderly, immutable world of computer memory. First, because all possible locations (bits) must be read in order to perform the lookup operation, only one pattern can be present in the network at one time. Furthermore, dense representations are very sensitive to noise. Each pattern of active locations represents a distinct memory, so if one the activity of one location is altered, the memory that is retrieved will be completely changed.
 
The substrate of the brain is very different from a computer: cells and synapses are constantly being modified or turned over, and the input cues that networks receive during memory retrieval are noisy. In this environment, the most effective coding scheme is sparse distributed memory, where information is spread across a sparse subset of units in a larger total population. At their most basic form, sparse distributed memory networks can be represented as very large sets of locations, with only a small percentage of those locations active during the representation of any one piece of information. Because there is so much ‘room’ in the set of locations, sparse codes can coexist in the same network. Furthermore, each individual location can be involved in the representation of multiple memories. Overlap between patterns can be used to encode associations between memories. Additionally, memories can be recalled from incomplete or corrupted input patterns as presenting the network with partial pattern is able to narrow the range of appropriate stored patterns to practical degrees of statistical certainty [@ahmad_properties_2015]. Sparse distributed representations also allow new patterns to be stored simply by altering the connectivity of existing units, such as in a neural network. No new addresses need to be created to store new patterns. Rather, the network merely needs to be modified enough to create a new pattern of activity that is sufficiently different from pre-existing patterns. These features, along with neurophysiological data showing sparse patterns of activity during memory retrieval in the amygdala [@bach_stable_2011] hippocampus [@wixted_sparse_2014] provide strong evidence for the use of sparse coding in the brain. 

### Modeling memory allocation in the lateral amygdala

In order to understand the mechanisms contributing to the induction and storage of emotional memories, a significant amount of work has been done to develop neural network models of the LA. One such model, introduced by D. Kim, Paré, & Nair, consists of an autoassociate neural network that contains both excitatory and inhibitory cells, implements Hebbian forms of long-term potentiation and depression, and replicates patterns of intra-LA connectivity discovered by systematic electrophysiological recording studies in the rat LA. By delivering input to the network corresponding to what would occur during auditory fear conditioning, the group has been able to replicate the findings of several electrophysiological and IEG-based studies and has gone on to make proposals about the mechanisms by which neurons are allocated to the engram during encoding.

A series of experimental work in mice has demonstrated that neuronal excitability plays a central role in biasing allocation to an engram [@han_neuronal_2007, @hsiang_manipulating_2014, @yiu_neurons_2014, @zhou_creb_2009].  However, artificially increased excitability of up to 20% of LA neurons in these studies did not increase the overall size of the of the LA component of the engram (in terms of number of neurons), suggesting that allocation is a competitive process. However, these studies were unable to explain how this process of competition occurs or what purpose it serves. In order to answer these questions, Kim et al. performed a series of experiments in their model, arbitrarily increasing the excitability of subsets of excitatory neurons and recording the effects on the size of the stable engram after repeated training in an auditory fear conditioning paradigm. Interestingly, they were able to replicate and extend the effect demonstrated in previous studies: increasing excitability in up to 25% of cells did not alter the size of the memory trace [@kim_mechanisms_2013-1]. Furthermore, with the control their model afforded them, they were able to demonstrate that inhibition is the primary factor in constraining neuronal allocation to an engram [@feng_mechanisms_2016]. Their model demonstrates that competitive interactions between principal neurons are mediated by disynaptic connections involving inhibitory interneurons. Essentially, highly excitable principal neurons activated during learning inhibit many of their principal neuron neighbors via indirect, inhibitory connections. Highly excitable principal neurons that share excitatory connections with each other become even more excited, while principal neurons that primarily receive disynaptic inhibitory connections from this highly excitable subset are silenced. This process magnifies the difference in excitability between ‘winners’ and ‘loser’ neurons. Furthermore, they demonstrate that plasticity at inhibitory synapses is able to compensate for increases in excitability such as would be seen during CREB overexpression.

This neural network modelling approach was also able to suggest why engrams in the LA are constrained. First, the sparse code enforced by competitive interactions is consistent with theoretical models of sparse distributed coding schemes that have also been demonstrated in other areas of the brain, such as hippocampus [@wixted_sparse_2014] and cortex [@gdalyahu_associative_2012] Thus, it reasonable to assume that the LA benefits from the many advantages of sparse distributed representations in a neural substrate (high capacity, robustness to noise). Second, the model revealed that sparsity enforced by competitive interactions could ensure an optimal degree of stimulus specificity [@kim_synaptic_2015]. By selectively modulating plasticity of either excitatory or inhibitory synapses, the researchers demonstrated that excitatory connectivity drives the number of cells allocated to the engram upwards, but increases the probability that memories will be generalized. Inhibitory connectivity had the opposite effect, decreasing the proportion of engram cells and making memories more specific. For an animal in the wild, the balance between memory specificity and generalizability is extremely important, drawing the line between failure to respond to threats and chronic anxiety. 

